fatal: destination path 'circuit-tracer' already exists and is not an empty directory.
Activating existing pyenv environment 'beer'
2025-12-18 16:43:18,854 - __main__ - INFO - ================================================================================
2025-12-18 16:43:18,855 - __main__ - INFO - Starting transcoder training
2025-12-18 16:43:18,855 - __main__ - INFO - ================================================================================
2025-12-18 16:43:18,904 - __main__ - INFO - Using device: cuda, with PyTorch version: 2.9.0+cu126, and CUDA version: 12.6
2025-12-18 16:43:18,905 - __main__ - INFO - CUDA devices available: 2
2025-12-18 16:43:18,933 - __main__ - INFO -   GPU 0: NVIDIA GeForce GTX 1080 Ti
2025-12-18 16:43:18,933 - __main__ - INFO -   GPU 1: NVIDIA GeForce GTX 1080 Ti
2025-12-18 16:43:18,933 - __main__ - INFO - Loading model and tokenizer: Babelscape/wikineural-multilingual-ner
2025-12-18 16:43:19,931 - __main__ - INFO - Model loaded. Model config: BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "dtype": "float32",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-PER",
    "2": "I-PER",
    "3": "B-ORG",
    "4": "I-ORG",
    "5": "B-LOC",
    "6": "I-LOC",
    "7": "B-MISC",
    "8": "I-MISC"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-LOC": 5,
    "B-MISC": 7,
    "B-ORG": 3,
    "B-PER": 1,
    "I-LOC": 6,
    "I-MISC": 8,
    "I-ORG": 4,
    "I-PER": 2,
    "O": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.57.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 119547
}

2025-12-18 16:43:19,931 - __main__ - INFO - Loading training data for model: Babelscape/wikineural-multilingual-ner
2025-12-18 16:43:20,439 - __main__ - INFO - Found 1 dataset(s): ['Babelscape/wikineural']
2025-12-18 16:43:20,441 - __main__ - INFO - Loading dataset: Babelscape/wikineural
2025-12-18 16:43:25,549 - __main__ - INFO - Loaded 1027506 samples from Babelscape/wikineural
2025-12-18 16:43:25,550 - __main__ - INFO - Total training samples: 1027506
2025-12-18 16:43:25,551 - __main__ - INFO - Processing input column data
2025-12-18 16:43:26,740 - __main__ - INFO - Data preparation complete. Final dataset size: 1027506
2025-12-18 16:43:26,740 - __main__ - INFO - Splitting dataset into train and evaluation sets
2025-12-18 16:43:27,086 - __main__ - INFO - Train samples: 924755, Eval samples: 102751
2025-12-18 16:43:31,013 - __main__ - INFO - Creating transducer configuration
2025-12-18 16:43:31,014 - __main__ - INFO - Transducer config: {'dtype': torch.float32, 'tie_word_embeddings': True, 'decoder_start_token_id': None, 'initializer_range': 0.02, 'n_ctx': 512, 'n_layers': 12, 'd_model': 768, 'd_head': 64, 'act_fn': 'gelu'}
2025-12-18 16:43:31,014 - __main__ - INFO - Initializing ReplacementModel (transducer)
2025-12-18 16:43:33,283 - __main__ - INFO - Creating LightningWrapper
2025-12-18 16:43:33,286 - __main__ - INFO - Initializing PyTorch Lightning Trainer
2025-12-18 16:43:33,286 - __main__ - INFO - Detected 2 GPUs. Attempting multi-GPU training with DDP.
/home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 transcoder_training.py --model-name Babelscape/wiki ...
Using bfloat16 Automatic Mixed Precision (AMP)
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
/home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
2025-12-18 16:43:33,346 - __main__ - INFO - Trainer configuration: max_epochs=100, precision=bf16-mixed, gradient_clip_val=1.0, strategy=ddp_find_unused_parameters_true, devices=auto
2025-12-18 16:43:33,347 - __main__ - INFO - Starting training...
/home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
2025-12-18 16:43:52,260 - __main__ - INFO - ================================================================================
2025-12-18 16:43:52,261 - __main__ - INFO - Starting transcoder training
2025-12-18 16:43:52,261 - __main__ - INFO - ================================================================================
2025-12-18 16:43:52,315 - __main__ - INFO - Using device: cuda, with PyTorch version: 2.9.0+cu126, and CUDA version: 12.6
2025-12-18 16:43:52,315 - __main__ - INFO - CUDA devices available: 2
2025-12-18 16:43:52,327 - __main__ - INFO -   GPU 0: NVIDIA GeForce GTX 1080 Ti
2025-12-18 16:43:52,328 - __main__ - INFO -   GPU 1: NVIDIA GeForce GTX 1080 Ti
2025-12-18 16:43:52,328 - __main__ - INFO - Loading model and tokenizer: Babelscape/wikineural-multilingual-ner
2025-12-18 16:43:53,608 - __main__ - INFO - Model loaded. Model config: BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "dtype": "float32",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-PER",
    "2": "I-PER",
    "3": "B-ORG",
    "4": "I-ORG",
    "5": "B-LOC",
    "6": "I-LOC",
    "7": "B-MISC",
    "8": "I-MISC"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-LOC": 5,
    "B-MISC": 7,
    "B-ORG": 3,
    "B-PER": 1,
    "I-LOC": 6,
    "I-MISC": 8,
    "I-ORG": 4,
    "I-PER": 2,
    "O": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.57.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 119547
}

2025-12-18 16:43:53,608 - __main__ - INFO - Loading training data for model: Babelscape/wikineural-multilingual-ner
2025-12-18 16:43:53,789 - __main__ - INFO - Found 1 dataset(s): ['Babelscape/wikineural']
2025-12-18 16:43:53,793 - __main__ - INFO - Loading dataset: Babelscape/wikineural
2025-12-18 16:43:58,168 - __main__ - INFO - Loaded 1027506 samples from Babelscape/wikineural
2025-12-18 16:43:58,170 - __main__ - INFO - Total training samples: 1027506
2025-12-18 16:43:58,171 - __main__ - INFO - Processing input column data
2025-12-18 16:43:59,345 - __main__ - INFO - Data preparation complete. Final dataset size: 1027506
2025-12-18 16:43:59,345 - __main__ - INFO - Splitting dataset into train and evaluation sets
2025-12-18 16:43:59,672 - __main__ - INFO - Train samples: 924755, Eval samples: 102751
2025-12-18 16:44:03,634 - __main__ - INFO - Creating transducer configuration
2025-12-18 16:44:03,634 - __main__ - INFO - Transducer config: {'dtype': torch.float32, 'tie_word_embeddings': True, 'decoder_start_token_id': None, 'initializer_range': 0.02, 'n_ctx': 512, 'n_layers': 12, 'd_model': 768, 'd_head': 64, 'act_fn': 'gelu'}
2025-12-18 16:44:03,634 - __main__ - INFO - Initializing ReplacementModel (transducer)
2025-12-18 16:44:05,906 - __main__ - INFO - Creating LightningWrapper
2025-12-18 16:44:05,909 - __main__ - INFO - Initializing PyTorch Lightning Trainer
2025-12-18 16:44:05,909 - __main__ - INFO - Detected 2 GPUs. Attempting multi-GPU training with DDP.
2025-12-18 16:44:06,119 - __main__ - INFO - Trainer configuration: max_epochs=100, precision=bf16-mixed, gradient_clip_val=1.0, strategy=ddp_find_unused_parameters_true, devices=auto
2025-12-18 16:44:06,120 - __main__ - INFO - Starting training...
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
/home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name      | Type                       | Params | Mode 
-----------------------------------------------------------------
0 | model     | ReplacementModel           | 297 M  | train
1 | ref_model | BertForTokenClassification | 177 M  | eval 
-----------------------------------------------------------------
297 M     Trainable params
177 M     Non-trainable params
474 M     Total params
1,899.208 Total estimated model params size (MB)
423       Modules in train mode
228       Modules in eval mode
[rank1]:[E1218 17:14:07.993230963 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15, OpType=BROADCAST, NumelIn=3145728, NumelOut=3145728, Timeout(ms)=1800000) ran for 1800069 milliseconds before timing out.
[rank1]:[E1218 17:14:07.996732374 ProcessGroupNCCL.cpp:2241] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 15 PG status: last enqueued work: 17, last completed work: 14
[rank1]:[E1218 17:14:07.996741961 ProcessGroupNCCL.cpp:730] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1218 17:14:07.996764469 ProcessGroupNCCL.cpp:2573] [PG ID 0 PG GUID 0(default_pg) Rank 1] First PG on this rank to signal dumping.
NCCL version 2.27.5+cuda12.9
[rank0]:[E1218 17:14:07.018358832 ProcessGroupNCCL.cpp:683] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=17, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800041 milliseconds before timing out.
[rank0]:[E1218 17:14:07.018412419 ProcessGroupNCCL.cpp:2241] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 17 PG status: last enqueued work: 17, last completed work: 16
[rank0]:[E1218 17:14:07.018418009 ProcessGroupNCCL.cpp:730] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E1218 17:14:07.018433692 ProcessGroupNCCL.cpp:2573] [PG ID 0 PG GUID 0(default_pg) Rank 0] First PG on this rank to signal dumping.
[rank1]:[E1218 17:14:07.384516005 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 17, last completed NCCL work: 14.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E1218 17:14:07.384573150 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 17, last completed NCCL work: 16.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E1218 17:14:07.400034821 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E1218 17:14:07.400145411 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank1]:[E1218 17:15:07.996836504 ProcessGroupNCCL.cpp:744] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E1218 17:15:07.997329979 ProcessGroupNCCL.cpp:758] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E1218 17:15:07.998489770 ProcessGroupNCCL.cpp:2057] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15, OpType=BROADCAST, NumelIn=3145728, NumelOut=3145728, Timeout(ms)=1800000) ran for 1800069 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f07d3d7cb80 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f077d3aa177 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x7f077d3aed81 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x7f077d3affcf in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f07d8b8f253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f07ddb36ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x1268c0 (0x7f07ddbc88c0 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=15, OpType=BROADCAST, NumelIn=3145728, NumelOut=3145728, Timeout(ms)=1800000) ran for 1800069 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f07d3d7cb80 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f077d3aa177 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x7f077d3aed81 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x7f077d3affcf in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f07d8b8f253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f07ddb36ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x1268c0 (0x7f07ddbc88c0 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f07d3d7cb80 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1335441 (0x7f077d386441 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xe56ae9 (0x7f077cea7ae9 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f07d8b8f253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f07ddb36ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x1268c0 (0x7f07ddbc88c0 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E1218 17:15:07.018509545 ProcessGroupNCCL.cpp:744] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E1218 17:15:07.018531229 ProcessGroupNCCL.cpp:758] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E1218 17:15:07.019702518 ProcessGroupNCCL.cpp:2057] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=17, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800041 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f7c9a57cb80 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f7c43faa177 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x7f7c43faed81 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x7f7c43faffcf in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f7c9f69b253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f7ca4642ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x1268c0 (0x7f7ca46d48c0 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=17, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800041 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f7c9a57cb80 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f7c43faa177 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x7f7c43faed81 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x7f7c43faffcf in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f7c9f69b253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f7ca4642ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x1268c0 (0x7f7ca46d48c0 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f7c9a57cb80 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1335441 (0x7f7c43f86441 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xe56ae9 (0x7f7c43aa7ae9 in /home/is/davide-r/.pyenv/versions/beer/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f7c9f69b253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f7ca4642ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x1268c0 (0x7f7ca46d48c0 in /lib/x86_64-linux-gnu/libc.so.6)

/var/lib/slurm/slurmd/job354671/slurm_script: line 52: 2075698 Aborted                 (core dumped) python3 transcoder_training.py --model-name Babelscape/wikineural-multilingual-ner
